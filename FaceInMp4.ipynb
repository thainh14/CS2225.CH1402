{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPDAm9DCcdmplLTO4nZySRw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vuvh87/CS2225.CH1402/blob/master/FaceInMp4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU0iNbGfYuGK"
      },
      "source": [
        "!git clone https://github.com/vuvh87/CS2225.CH1402.git vra\n",
        "!mkdir Video\n",
        "!mkdir Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2iNdbC1aWHm"
      },
      "source": [
        "!pip install face_recognition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTXcYTAuY3Kg"
      },
      "source": [
        "import face_recognition\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SVsjyz9Y5hF"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0ndBBfcZCh9"
      },
      "source": [
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "def playvideo(filename):\n",
        "    video = io.open(filename, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    return HTML(data='''<video alt=\"test\" controls>\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
        "                 </video>'''.format(encoded.decode('ascii')))\n",
        "#Now play...\n",
        "playvideo('/content/drive/My Drive/Colab Notebooks/seth_godin_interview_small.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0jMk4mqZE7t"
      },
      "source": [
        "input_video = cv2.VideoCapture(\"/content/drive/My Drive/Colab Notebooks/seth_godin_interview_small.mp4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFO_MSauZG_-"
      },
      "source": [
        "# Metadata from the input video\n",
        "frames_per_second = int(input_video.get(cv2.CAP_PROP_FPS))\n",
        "frame_width = int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "print('Metadata from input video:',\n",
        "      '\\nFrames per second:', frames_per_second,\n",
        "      '\\nFrame width:', frame_width, \n",
        "      '\\nFrame height:', frame_height)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz-h-pKTZIpF"
      },
      "source": [
        "codec = cv2.VideoWriter.fourcc(*'XVID')\n",
        "video_writer = cv2.VideoWriter('/content/drive/My Drive/Colab Notebooks/output_video.mp4', \n",
        "                               codec, \n",
        "                               frames_per_second, \n",
        "                               (frame_width, frame_height))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuJhrk3aZK_2"
      },
      "source": [
        "# OpenCV Default config file for face detection\n",
        "haar_file = '/content/drive/My Drive/Colab Notebooks/haarcascades/haarcascade_frontalface_default.xml'\n",
        "  \n",
        "# Root Folder used to store the face data images\n",
        "datasets = '/content/drive/My Drive/Colab Notebooks/Dataset' \n",
        "\n",
        "# define the size of images  \n",
        "(width, height) = (130, 100)     \n",
        "  \n",
        "# Analyse the video training file \n",
        "face_cascade = cv2.CascadeClassifier(haar_file) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdgboIgNZOc-"
      },
      "source": [
        "# An array to hold the locations of faces that are detected on individual frames\n",
        "face_locations = []\n",
        "\n",
        "# A counter to keep track of the number of frames processed\n",
        "count = 1\n",
        "\n",
        "# Loop through all the frames in the video\n",
        "while (count != 1000):\n",
        "  # Read the video to retrieve individual frames. 'frame' will reference the inidivdual frames read from the video.\n",
        "  ret, frame = input_video.read()\n",
        "\n",
        "  # Check the 'ret' (return value) to see if we have read all the frames in the video to exit the loop\n",
        "  if not ret:\n",
        "    print('Processed all frames')\n",
        "    break\n",
        "\n",
        "  # Convert the image (frame) to RGB format as by default Open CV uses BGR format. \n",
        "  # This conversion is done as face_recognition and other libraries usually use RGB format.\n",
        "  rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "  \n",
        "  # Get the coordinates in the image where a face is detected. Use the model 'cnn' after greater accuracy.\n",
        "  face_locations = face_recognition.face_locations(rgb_frame, model='cnn')\n",
        "\n",
        "  # Loop through the face locations array and draw a rectangle around each face that is detected in the frame\n",
        "  for top, right, bottom, left in face_locations:\n",
        "    cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
        "\n",
        "  # Write the frame to the output vide0\n",
        "  video_writer.write(frame)\n",
        "  \n",
        "  #cv2_imshow(frame)\n",
        "  \n",
        "  # Print for every 50 frames processed\n",
        "  if(count % 50 == 0):\n",
        "    print('Processed', count, 'frames')\n",
        "\n",
        "  count += 1\n",
        "\n",
        "  #Vu\n",
        "  faces = face_cascade.detectMultiScale(rgb_frame, 1.3, 4)\n",
        "  for (x, y, w, h) in faces: \n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2) \n",
        "        face = rgb_frame[y:y + h, x:x + w] \n",
        "        face_resize = cv2.resize(face, (width, height)) \n",
        "        cv2.imwrite('% s/% s.png' % (datasets, count), face_resize) \n",
        "\n",
        "# Release to close all the resources that we have opened for reading and writing video\n",
        "input_video.release()\n",
        "video_writer.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}